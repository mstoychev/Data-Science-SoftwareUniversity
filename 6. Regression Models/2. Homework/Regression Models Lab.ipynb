{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nose.tools import *\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Models Lab\n",
    "## Logistic regression: problem statement, derivation, usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification: Problem Statement\n",
    "In many real cases, the output variable is categorical, i.e. our model should return one of some predefined categories.\n",
    "\n",
    "There are a lot of examples:\n",
    "* Classifying images\n",
    "* Classifying diseases (is a given patient healthy or ill?)\n",
    "* Any case of \"good / bad\" classification\n",
    "* Anomaly detection (e.g. credit card fraud detection)\n",
    "* Processes which involve creating catalogs, etc.\n",
    "\n",
    "We'll start with the simplest case:\n",
    "* Only one input variable $x$\n",
    "* Binary output variable $y$, i.e. either $y = 0$ or $y = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1. Generate Some Sample Data (1 point)\n",
    "Let's imagine the simplest possible case. For example, we can think of $x$ as \"hours of studying time\" and $y$ as \"passed exam\" (0 of false, 1 if true).\n",
    "\n",
    "A class has 20 students. 12 of them studied between 1 and 3 hours and didn't pass the exam. The other 8 studied between 7 and 10 hours and passed the exam. Let's model this situation.\n",
    "\n",
    "First, to make our work easier, we can split it into two: for failed and passed students. Each student studied a random time, so let's choose this from a random uniform distribution (hint: `np.random.uniform(min, max, size)`).\n",
    "\n",
    "Create the following:\n",
    "```python\n",
    "failed_student_times = np.random.uniform(?, ?, ?)\n",
    "passed_student_times = np.random.uniform(?, ?, ?)\n",
    "all_times = np.concatenate([failed_student_times, passed_student_times])\n",
    "```\n",
    "\n",
    "Now, let's generate the outcome variable: we need 12 zeros, followed by 8 ones.\n",
    "```python\n",
    "exam_result = ...\n",
    "```\n",
    "\n",
    "**Note:** We don't need to use `numpy` arrays but they will give us many benefits later. One is that our code will be really easy and short, and another is that they are very fast (in terms of computation time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1926e09cac787971e16133423f28901f",
     "grade": false,
     "grade_id": "cell-9c723e1939fc3177",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "failed_student_times = np.random.uniform(1, 3, size=12)\n",
    "passed_student_times = np.random.uniform(7, 10, size=8)\n",
    "all_times = np.concatenate([failed_student_times, passed_student_times])\n",
    "\n",
    "exam_result = np.concatenate((np.zeros(12, dtype=int), np.ones(8, dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dc76662f57db4d8fbbe160bed101ec6f",
     "grade": true,
     "grade_id": "cell-77dbf808a1024c06",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(all_times, np.ndarray)\n",
    "assert_is_instance(exam_result, np.ndarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now plot the student performance, we'll get something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_exam_results():\n",
    "    plt.scatter(all_times, exam_result)\n",
    "    plt.xlabel(\"Study time (hours)\")\n",
    "    plt.ylabel(\"Exam result\")\n",
    "    plt.yticks([0, 1])\n",
    "    plt.show()\n",
    "plot_exam_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a very obvious distinction between the two classes. Once again, that's because we wanted so.\n",
    "\n",
    "How can we model the data? An example would be:\n",
    "```python\n",
    "if x < 5: # or x < 6, or something like that\n",
    "    return 0\n",
    "else:\n",
    "    return 1\n",
    "```\n",
    "\n",
    "This model works but let's look at a more realistic scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2. Generate More Realistic Sample Data (1 point)\n",
    "Now, this was a really simple view. A more involved model would include some error.\n",
    "\n",
    "First, let's ensure the test results are repeatable, even with random variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reuse the previous example but this time:\n",
    "* Generate 20 study times between 1 and 10. Choose each number randomly from a uniform distribution (hint: this is the same as what you did before).\n",
    "* Generate the exam output variable like this: For each study time, if it's $\\le$ 3 hours, it should be 0, if it's $\\ge$ 7 hours, it should be 1. If the study time is between 3 and 7 hours, decide randomly whether it should be 0 or 1.\n",
    "* How do we decide randomly between 0 and 1? A simple way would be to generate a random number between 0 and 1: `np.random.random()`. If that number is >= 0.5, say the student passed the exam and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e676611d84d33585bdbc288476c4abcc",
     "grade": false,
     "grade_id": "cell-9740949f4386388b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "all_times = None\n",
    "exam_result = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "84402a75e10ed6dacd326da0ada9473c",
     "grade": true,
     "grade_id": "cell-56a1a993a1fe0b84",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(all_times, np.ndarray)\n",
    "assert_is_instance(exam_result, np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_exam_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the results look more fuzzy. It's obvious that we can't model them with 100% accuracy.\n",
    "\n",
    "Still, let's try some things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3. Decide on a Modelling Function (2 points)\n",
    "We can see that our old approach can work somewhat. If we try to plot the prediction function, we'll see this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(all_times, exam_result)\n",
    "plt.hlines([0, 1], [1, 5], [5, 10], color = \"red\")\n",
    "plt.vlines(5, 0, 1, linestyle = \"dashed\", color = \"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red line is called \"decision boundary\". We can see that we misclassified some students but we are mostly correct.\n",
    "\n",
    "However, the function has problems. First of all, it's undefined at $x = 5$ (we don't know if it's 0 or 1). Second, it has \"sharp corners\", and mathematicians hate functions with sharp corners :).\n",
    "\n",
    "We're looking for a function that kind of looks like our line. And there is such a function. It's called a **sigmoid** function. Its definition is like this:\n",
    "\n",
    "$$ \\sigma(z) = \\frac{1}{1+e^{-z}} $$\n",
    "\n",
    "Implement the previous formula in code. Note: you can use `np.exp(something)` instead of `np.e ** something` - it's much more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "79cce4179f40f32dc70c0beed97bd359",
     "grade": false,
     "grade_id": "cell-5a833837981bc4a0",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the function (and hopefully it's correct :D), we can plot it. The plot should look similar to what's described in [this](https://en.wikipedia.org/wiki/Sigmoid_function) article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 1000)\n",
    "y = sigmoid(x) # Note that this line should work correctly\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function, as you can see, has interesting properties:\n",
    "* For really large negative $x$ (say $x < -5$), it's practically 0\n",
    "* For really large positive $x$, it's practically 1\n",
    "* At 0, it's 0.5 exactly\n",
    "\n",
    "Also, it looks like our original guess and has no sharp corners. This function is also called **logistic function** and it will be our **modelling function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7fe48a38476e895d11e6c90914095c8f",
     "grade": true,
     "grade_id": "cell-fe10135ce3b8cff3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_almost_equal(sigmoid(-50), 0, delta = 1e-10)\n",
    "assert_almost_equal(sigmoid(0), 0.5, delta = 1e-10)\n",
    "assert_almost_equal(sigmoid(50), 1, delta = 1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to fix an issue: $\\sigma(z)$ has continuous output, as we already saw. How do we make it binary?\n",
    "\n",
    "Simple enough; if $\\sigma \\lt 0.5$, we'll output 0 and 1 otherwise (note that this will assign class 1 to $\\sigma(0)$ but that's OK).\n",
    "\n",
    "Let's write a function that accepts a $z$ (number between 0 and 1) and decides what output label should it produce: 0 or 1. Note that this is exactly the process that we defined as \"thresholding\" some time ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0a495a6adcb465a2a0701dbfa84eeb3e",
     "grade": false,
     "grade_id": "cell-72d18afc1bdbc4cd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_output(z):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c8de1dbddc706ac3ffa6b6da737431c9",
     "grade": true,
     "grade_id": "cell-e6e5fb40e0bb020f",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "for x in np.arange(-2, 3):\n",
    "    print(x, sigmoid(x), generate_output(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4. Use the Sigmoid Function as a Model (1 point)\n",
    "Now, we've got another issue ahead. How can we model the parameter $z$ of $\\sigma(z)$? All we know is that it should be a number.\n",
    "\n",
    "Well, this number must be somehow related to the input parameters. The simplest possible relation is linear:\n",
    "\n",
    "$$z = ax + b$$\n",
    "\n",
    "where $x$ is the number of hours, $a$ is its coefficient, and $b$ is an intercept (also called \"free term\" or \"bias term\").\n",
    "\n",
    "Therefore, we can rewrite our function $\\sigma(z)$ to be $l(x)$, like so:\n",
    "$$ l(x) = \\frac{1}{1+e^{-(ax+b)}}=\\sigma(ax+b) $$\n",
    "\n",
    "$l$ stands for \"logistic function\".\n",
    "\n",
    "This function has two properties:\n",
    "* Its value will tell us which class to choose (0 or 1)\n",
    "* It can serve as a probability: $l(x)$ is equal to the probability that the output class is 1\n",
    "\n",
    "Write a function that does exactly this. **Reuse your implementation** of the `sigmoid` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5fb407862fc4e5a8b3bdc4dacbc39488",
     "grade": false,
     "grade_id": "cell-671cdc824b7d585c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def logistic_function(x, a, b):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "df0ce821e8fff8db059aa511d6bc8049",
     "grade": true,
     "grade_id": "cell-92f8ba87e6c6ee5e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# The function should work with both numbers and numpy arrays\n",
    "print(logistic_function(0, 1, 0)) # Should be 0.5\n",
    "print(logistic_function(np.arange(-2, 5), a = -2, b = 3)) # Should return decreasing numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the problem is similar to what we did before: we need to find the best parameters $a$ and $b$.\n",
    "\n",
    "Let's try out different values of $a$ and $b$, just to get a feel of what we need to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_with_sigmoid(a, b, x, y, title = \"\"):\n",
    "    plt.scatter(x, y)\n",
    "    logistic_x_values = np.linspace(-2, 12, 1000)\n",
    "    logistic_y_values = logistic_function(logistic_x_values, a, b)\n",
    "    plt.plot(logistic_x_values, logistic_y_values, color = \"red\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "for a, b in [[2, 5], [3, -6], [-5, 0.5], [1.5, -8]]:\n",
    "    plot_prediction_with_sigmoid(a, b, all_times, exam_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can understand that $a$ controls how \"spread out\" the function is, also if $a < 0$, the function is inverted. $b$ seems to control where the \"center point\" is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5. Choose an Error Function (2 points)\n",
    "Now that we have an idea what our modelling function does, the next step is to choose a good error function.\n",
    "\n",
    "The error function should accept a single predicted value $\\tilde{y} = \\sigma(ax+b)$ and an actual value $y$. It should tell us whether we're right or not. Note that the sigmoid value is always between 0 and 1.\n",
    "\n",
    "**Note:** In terminology, there's a difference between \"error function\" and \"cost / loss function\". The error function is defined for one point only while the cost function is for the entire dataset.\n",
    "\n",
    "Since we have two classes, we will need to define a separate error function for each class: $error_0$ will correspond to the case where the correct class should be $0$; $error_1$ will correspond to class $1$.\n",
    "\n",
    "Our intuition is that the errors should be equal to zero if we predicted the class correctly, and greater than zero otherwise. So, for example, the error functions might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_values = np.linspace(0, 1, 1000)\n",
    "error_0_values = sigmoid_values\n",
    "error_1_values = - sigmoid_values + 1\n",
    "plt.plot(sigmoid_values, error_0_values, label = \"$error_0$\")\n",
    "plt.plot(sigmoid_values, error_1_values, label = \"$error_1$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take $error_0$ for example. It is 0 if the predicted class is 0 and increases everywhere else.\n",
    "\n",
    "There are many choices for error functions. However, not all are created equal. For purposes we're not going to discuss, it turns out the best error function for logistic regression is this:\n",
    "$$\n",
    "error_0 = -\\ln(1-x) \\\\\n",
    "error_1 = -\\ln(x)\n",
    "$$\n",
    "\n",
    "If you're interested why this function is the best, you can look it up online. The main idea is that this function has only one minimum. When we do gradient descent, we can get stuck in a local minimum and miss the global one. This error function ensures that nasty thing won't happen.\n",
    "\n",
    "Let's implement and inspect the two functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9ecbdc8a9fa75771cf373aa8c9b2429b",
     "grade": false,
     "grade_id": "cell-b8c755eadd32790e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def error_0(x):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def error_1(x):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2ebfe9f3d7be1711793bd74df9fa7566",
     "grade": true,
     "grade_id": "cell-19b0edfed3b62042",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_almost_equal(error_0(0), 0)\n",
    "assert_greater(error_0(1), 0)\n",
    "\n",
    "assert_almost_equal(error_1(1), 0)\n",
    "assert_greater(error_1(0), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will produce errors because log(0) is infinity; we can just ignore them for now\n",
    "sigmoid_values = np.linspace(0, 1, 1000)\n",
    "plt.plot(sigmoid_values, error_0(sigmoid_values), label = \"$error_0$\")\n",
    "plt.plot(sigmoid_values, error_1(sigmoid_values), label = \"$error_1$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These error functions not only work. They have an **additional property** that makes them special. Focus on $error_0$ for  example. Besides being $0$ when the predicted class is $0$, it's also equal to $\\infty$ when we get the class totally wrong.\n",
    "\n",
    "That's how you punish an algorithm for being wrong :)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more question: how do we decide whether we should use $error_0$ or $error_1$? We look at the original $y$ (`exam_result` in our case). If $y = 0$, we'll use $error_0$ and vice versa. This will combine our two error functions into one convenient error function.\n",
    "\n",
    "Write this convenient function. It should accept an input value $z \\in (-\\infty; \\infty)$ and an expected output value $y$ (0 or 1).\n",
    "\n",
    "First, it should pass $z$ through the sigmoid function to get $\\sigma(z)$. After that, based on $y$, it should choose the correct function and return the error.\n",
    "* If $y = 0$, return $error_0(\\sigma(z))$\n",
    "* If $y = 1$, return $error_1(\\sigma(z))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cc16f57595525e68013d968698083505",
     "grade": false,
     "grade_id": "cell-d34ce23b84cd1dcc",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_error(z, y):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this function to see that it works. If we pass a negative value for $z$, the sigmoid function will be approximately zero, so it predicts class 0. If $y = 0$, congrats, we have almost zero error. If $y = 1$, we'll get punished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "89062b5cdfbcf4dba13ac0e4f80d59e4",
     "grade": true,
     "grade_id": "cell-78e4aee5f7010ec8",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(get_error(-10, 0)) # Almost zero\n",
    "print(get_error(-10, 1)) # Very large (approximately equal to 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6. Calculate the Cost Function from the Error Function (1 point)\n",
    "Now, we have an error function which accepts a real number $z$ and an expected class and returns the prediction error. Now we have to repeat what we did in step 4 and rewrite the entire thing to use $x$, the input value.\n",
    "\n",
    "That's really simple. We should just take $x, y$, and the parameters $a, b$. All four parameters are real numbers. From them, we need to calculate $z = ax+b$. This $z$ is what we need to pass to `get_error(z, y)`.\n",
    "\n",
    "Write a function that does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4785d979c0b59107942400db95703c4d",
     "grade": false,
     "grade_id": "cell-7da0ed8e17557d5e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_error_function(a, b, x, y):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "print(compute_error_function(-5, 0, 1, 0)) # Should be about 0.007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total cost function is the sum of all errors. Write a function that accepts `data_x` and `data_y` - arrays of points, and also `a` and `b`. For each point, compute the error function. Sum all error functions to get the total cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "350e8521e233ebcc96caa3edce3efcf0",
     "grade": false,
     "grade_id": "cell-f6a7f0ad3bb7f5a8",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_total_cost(a, b, data_x, data_y):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go back and see what total costs we should get. We saw that the last function approximation looked most reasonable. It should have the smallest error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f808a100908c88a00becb0ea81a7e02a",
     "grade": true,
     "grade_id": "cell-aefbf00d7ba63fa2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "for a, b in [[2, 5], [3, -6], [-5, 0.5], [1.5, -8]]:\n",
    "    total_cost = compute_total_cost(a, b, all_times, exam_result)\n",
    "    plot_prediction_with_sigmoid(a, b, all_times, exam_result, \"Total cost: \" + str(total_cost))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7. Perform gradient ascent (1 point)\n",
    "Now we've gone really far. What remains is to try to find the unknown coefficients $a$ and $b$.\n",
    "\n",
    "We're not going to cover the details of the derivation of the gradients. If you're interested how this works, you can look it up online. The method is fairly common in statistics and is called \"maximum likelihood estimation\", i.e. we try to estimate the parameters $a$ and $b$ so that the prediction $\\tilde{y}$ is as close as possible to the observed variable $y$.\n",
    "\n",
    "Here's an important detail, though. Because of how these calculations go, we need to **maximize**, not minimize the target function. The method is called **gradient ascent**. When we update the values, we add, not subtract, the new gradients.\n",
    "\n",
    "$$\n",
    "a = a + \\nabla J_a \\\\\n",
    "b = b + \\nabla J_b\n",
    "$$\n",
    "\n",
    "The implementation of `perform_gradient_ascent()` for logistic regression is given to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_gradient_ascent(a, b, data_x, data_y, learning_rate):\n",
    "    y_predicted = sigmoid(a * data_x + b)\n",
    "    errors = data_y - y_predicted\n",
    "    a_gradient = np.sum(data_x * errors)\n",
    "    b_gradient = np.sum(errors)\n",
    "    new_a = a + learning_rate * a_gradient\n",
    "    new_b = b + learning_rate * b_gradient\n",
    "    return new_a, new_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use that to perform 3 000 iterations. Start from $a = 0,\\ b = 0$. At each iteration, update $a$ and $b$. Print the final values at the end. Use `all_times` and `exam_result` as your variables. Use a learning rate $\\alpha = 0.01$.\n",
    "\n",
    "If you wish, you can call `compute_total_cost(a, b, data_x, data_y)` and even plot the learning curve. This will give you a good idea whether your model worked or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bf029cf1087c2b1cc96548feb87b9311",
     "grade": false,
     "grade_id": "cell-1486b3cbc3fe053c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def train_model(data_x, data_y):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = train_model(all_times, exam_result)\n",
    "print(\"a = {}; b = {}\".format(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e9ea50d469d2f1288a8152045cd0a205",
     "grade": true,
     "grade_id": "cell-9cd155d1d279863a",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_not_none(a)\n",
    "assert_is_not_none(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare our implementation to something else. An algorithm that we know works for sure, is the `scikit-learn` implementation.\n",
    "\n",
    "The next cell performs logistic regression using `scikit-learn`. You should observe that the parameters are almost the same as those we got. This means that our implementation works well.\n",
    "\n",
    "The `C` parameter is related to something we haven't discussed called **regularization**. A large number means no regularization is applied, which is what we did. A more detailed discussion is out of the scope of this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(C = 1e9)\n",
    "model.fit(all_times.reshape(-1, 1), exam_result)\n",
    "print(\"a = {}; b = {}\".format(model.coef_[0, 0], model.intercept_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8. Test your model (1 point)\n",
    "Let's create a `predict()` function. It should accept `a` and `b`, and the training data (`all_times` in our case). Its job is to predict the output class. To do this, it should use the already defined `generate_output()` function. To make things easier, this function is provided for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(a, b, x_data):\n",
    "    return generate_output(a * x_data + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now call `predict()` for every input value and compare our outputs to the original ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_result_predicted = np.array([predict(a, b, time) for time in all_times])\n",
    "print(\"Predicted:\", exam_result_predicted)\n",
    "print(\"Actual:   \", exam_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A metric for how good our classification is, is called **accuracy**. It's the ratio of guessed classes to all classes.\n",
    "\n",
    "$$A = \\frac{\\text{number of correct guesses}}{\\text{number of total results}}$$\n",
    "\n",
    "Write a function called `calculate_accuracy` which accepts the predicted exam results and the actual exam results and returns the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "99d83a7c543363fe521f1d88c8946c9f",
     "grade": false,
     "grade_id": "cell-321117662720ef67",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(predicted, actual):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "62034ce5d5171f3e19f1ea3af2b973a5",
     "grade": true,
     "grade_id": "cell-5cdae793298156a5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(calculate_accuracy(exam_result_predicted, exam_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if we want to play a little, we can plot correct and incorrect results. This is left as an exercise to the reader :).\n",
    "\n",
    "### Additional notes: OOP\n",
    "We're not looking at object-oriented programming in this course, but we can wrap everything we did in a class. I am including this just for fun. The class contains only vary basic input checks. A real-life scenario will include much more validation.\n",
    "\n",
    "Also, in this class, I'm calling the functions `train_model()`, `predict()`, and `calculate_accuracy()` directly because I know they're already written. A real class won't simply reuse them, they will be part of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneVariableLogisticRegression(object):\n",
    "    def __init__(self):\n",
    "        self.is_trained = False\n",
    "        \n",
    "    def train(self, x, y):\n",
    "        assert_is_instance(x, np.ndarray)\n",
    "        assert_is_instance(y, np.ndarray)\n",
    "        assert_equal(x.ndim, 1)\n",
    "        assert_equal(y.ndim, 1)\n",
    "        \n",
    "        self.is_trained = True\n",
    "        self.a, self.b = train_model(x, y)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        if not self.is_trained:\n",
    "            raise RuntimeError(\"The model is not trained\")\n",
    "        return np.vectorize(predict)(self.a, self.b, x)\n",
    "    \n",
    "    def score(self, y_predicted, y_actual):\n",
    "        return calculate_accuracy(y_predicted, y_actual)\n",
    "    \n",
    "# Usage\n",
    "model = OneVariableLogisticRegression()\n",
    "model.train(all_times, exam_result)\n",
    "print(\"a = {}; b = {}\".format(model.a, model.b))\n",
    "print(\"Predicted:\", model.predict(all_times))\n",
    "print(\"Accuracy: {0:.3f}\".format(model.score(model.predict(all_times), exam_result)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
